import keras
from keras.layers import Input, LSTM, Dense, Embedding
from keras.models import Model
from keras.preprocessing.sequence import pad_sequences
import io
from nltk import word_tokenize 
import numpy as np
# This is an attempt to solve semantic textual similarity 
# Benchmark semeval data is taken..training and testing is done using this data itself
# trian, dev, and test data contain 5749 1500 1379 sentence pairs respectively 
trainData = "/home/star/Desktop/github_projects/final/semantic_textual_similarity/stsbenchmark/sts-train.csv"
devData = "/home/star/Desktop/github_projects/final/semantic_textual_similarity/stsbenchmark/sts-dev.csv"
testData = "/home/star/Desktop/github_projects/final/semantic_textual_similarity/stsbenchmark/sts-test.csv"
glove = "/home/star/data/glove/glove.6B.50d.txt"
VECTOR_LEN = 50
THRESHOLD = 20

def preprocess(trainData,THRESHOLD) :
    # Preprocess the text and store in a list format
    # Each item in the list is a list containing score,sentence1,sentence2 in that order
    train = []
    with io.open(trainData, encoding="utf-8") as f:
        lines = f.readlines()#decode("utf-8")
        #print lines 
        for line in lines:
            line = line.split('\t')
            for sent in line:
                sent = sent.strip()
            train.append(line[4:7]) 

    # Removing the list elements which have both sentences' length above a threshold 
    count = 0
    new = []
    # iterating through the reversed list 
    for i in range(len(train))[::-1] :
        sent1 = word_tokenize(train[i][1])
        sent2 = word_tokenize(train[i][2])
        if (len(sent1) <= THRESHOLD and len(sent2)<=THRESHOLD):
            output = float(train[i][0])
            new.append([output, sent1, sent2])

    return new  


# Create a dictionary with ids for all the words/tokens in the corpus
# Giving a common id for all the words not found in glove 
def dictionary(train,embedding_index):
    idDict = {}
    for item in train:
        #print item 
        for token in item:
            #print token 
            # if token doesnt exist in the dictionary
            if idDict.get(token) is None:
                # if its an UNKNOWN word, assign id -1 to it 
                if embedding_index.get(token) is None:
                    idDict[token] = -1
                # if its a word that exists in glove  
                else :
                    if len(idDict) == 0:
                        idDict[token] = 1
                    else:
                        k = max(idDict, key=idDict.get)
                        # k is the key of the highest value in the list 
                        highestId = idDict[k]
                        if highestId >= 1 :
                            idDict[token] = highestId + 1
                        else :
                            idDict[token] = 1

    # All the unknown words are given id -1 , now lets replace it with highestvalue+1
    maxKey = max(idDict, key=idDict.get)
    highest = idDict[maxKey]
    for word,value in idDict.items():
        if value == -1:
            idDict[word] = highest+1

    return idDict

def convert(train,idDict):
    for i,item in enumerate(train):
        #print item 
        for j,token in enumerate(item):
            #print token 
            train[i][j] = idDict[token]
    return train



# main code
train = preprocess(trainData,THRESHOLD)
test = preprocess(testData,THRESHOLD)
# getting the outputs and token lists 
trainOP = [item[0] for item in train]
train_a = [item[1] for item in train]
train_b = [item[2] for item in train]
testOP = [item[0] for item in test]
test_a = [item[1] for item in test]
test_b = [item[2] for item in test]

#print trainOP 
#print train_a

# Loading the glove embeddings into a dictionary with word as key and embedding as value
f = open(glove)
embedding_index = {}
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32') 
    embedding_index[word] = coefs 
f.close()
#creating a tokenlist 
tokenlist = train_a+train_b + test_a + test_b
#print tokenlist 
idDict = dictionary(tokenlist,embedding_index)
# convert the tokens to ids 
idtrain_a = convert(train_a,idDict)
idtrain_b = convert(train_b,idDict)
idtest_a = convert(test_a, idDict)
idtest_b = convert(test_b,idDict)
# padding 
idtrain_a = pad_sequences(idtrain_a,maxlen=THRESHOLD,padding='pre',truncating= 'post', value=0.0)
idtrain_b = pad_sequences(idtrain_b,maxlen=THRESHOLD,padding='pre',truncating= 'post', value=0.0)
idtest_a = pad_sequences(idtest_a,maxlen=THRESHOLD,padding='pre',truncating= 'post', value=0.0)
idtest_b = pad_sequences(idtest_b,maxlen=THRESHOLD,padding='pre',truncating= 'post', value=0.0)
# creating the embedding matrix 
maxKey = max(idDict, key=idDict.get)
highestValue = idDict[maxKey]
VOCAB_SIZE = highestValue +1 
embedMatrix = np.zeros((VOCAB_SIZE,VECTOR_LEN))
vector = np.random.rand(VECTOR_LEN)
for key,value in idDict.items():
    if value != 0 :
        embed = embedding_index.get(key)
        if embed is None :
            embedMatrix[value] = vector 
        else :
            embedMatrix[value] = embed 

#model comstruction using keras funcitonal api shared layers  

input_a = Input(shape=(THRESHOLD,))
input_b = Input(shape=(THRESHOLD,))
# This embedding layer will encode the input sequence
# into a sequence of dense THRESHOLD-dimensional vectors.
x = Embedding(output_dim=VECTOR_LEN, input_dim=VOCAB_SIZE, input_length=THRESHOLD,weights =[embedMatrix])
# getting the embeddings of both sentences
embed_a = x(input_a)
embed_b = x(input_b)
# This layer can take as input a matrix and will return a vector of size 64
shared_lstm = LSTM(64)
# getting the sentence encoding of both the sentences
encode_a = shared_lstm(embed_a)
encode_b = shared_lstm(embed_b)
# We can then concatenate the two vectors:
merged_vector = keras.layers.concatenate([encode_a, encode_b], axis=-1)
# And add a logistic regression on top
dense1 = Dense(70, activation='relu')(merged_vector)
dense2 = Dense(40,activation='relu')(dense1)
dense3 = Dense(10,activation='relu')(dense2)
predictions = Dense(1,activation='sigmoid')(dense3)
# We define a trainable model linking the sentence inputs to the predictions
model = Model(inputs=[input_a, input_b], outputs=predictions)
model.compile(optimizer='rmsprop',loss='mean_squared_error',metrics=['accuracy'])
model.fit([idtrain_a, idtrain_b], trainOP, epochs=10)
result = model.predict([idtest_a,idtest_b],verbose=0)
#print result[:200]
f1 = open("sts_test.txt","w")
f2 = open("my_op.txt","w")

for i in range(len(testOP)) :
        item1 = testOP[i]
        item2 = result[i]
        print >>f1, item1
        print >>f2, item2[0]

#loss,score = model.evaluate([idtest_a,idtest_b],testOP, verbose=1)
#print 'accuracy : ',score*100

































